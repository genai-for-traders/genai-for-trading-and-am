{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535a1c69",
   "metadata": {
    "id": "535a1c69"
   },
   "source": [
    "# Hugging Face Introduction\n",
    "\n",
    "Welcome to the hugging face tutorial, we will cover most of the important functionality of huggingface to train, inference and deploy models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798321c2",
   "metadata": {
    "id": "798321c2"
   },
   "source": [
    "## Inference with pipelines\n",
    "\n",
    "one of the things I love most about hugging face is that we don't need to completely understand the code behind models, the details about the architecture of the model or have experience with the modality in order to use any model.  \n",
    "\n",
    "\n",
    "the ```pipeline()```function let us inference any model from the huggingface hub on any modality such as computer vision, speech, text or multimodel tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02cd7aa",
   "metadata": {
    "id": "d02cd7aa"
   },
   "source": [
    "we will start by importing the ```pipeline``` function from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12bdae79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T12:42:14.753228Z",
     "start_time": "2024-09-10T12:42:05.219351Z"
    },
    "id": "12bdae79"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5231cf37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T15:40:59.444128Z",
     "start_time": "2024-09-08T15:40:59.332672Z"
    },
    "id": "5231cf37"
   },
   "source": [
    "The code below shows a snapshot of the parameters of the pipeline function.\n",
    "\n",
    "```python\n",
    "pipeline(\n",
    "    task: str = None,\n",
    "    model: Union[str, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel'), NoneType] = None,\n",
    "    config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None,\n",
    "    tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, ForwardRef('PreTrainedTokenizerFast'), NoneType] = None,\n",
    "    feature_extractor: Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None,\n",
    "    image_processor: Union[str, transformers.image_processing_utils.BaseImageProcessor, NoneType] = None,\n",
    "    framework: Optional[str] = None,\n",
    "    revision: Optional[str] = None,\n",
    "    use_fast: bool = True,\n",
    "    token: Union[str, bool, NoneType] = None,\n",
    "    device: Union[int, str, ForwardRef('torch.device'), NoneType] = None,\n",
    "    device_map=None,\n",
    "    torch_dtype=None,\n",
    "    trust_remote_code: Optional[bool] = None,\n",
    "    model_kwargs: Dict[str, Any] = None,\n",
    "    pipeline_class: Optional[Any] = None,\n",
    "    **kwargs,\n",
    ") -> transformers.pipelines.base.Pipeline\n",
    "\n",
    "```\n",
    "\n",
    "the ```task``` parameter specifies the specific pipeline we want to use. Below there is an non exhaustive list of all available tasks implemented by hf.\n",
    "```\n",
    "\"audio-classification\": will return a AudioClassificationPipeline.\n",
    "\"automatic-speech-recognition\": will return a AutomaticSpeechRecognitionPipeline.\n",
    "\"depth-estimation\": will return a DepthEstimationPipeline.\n",
    "\"document-question-answering\": will return a DocumentQuestionAnsweringPipeline.\n",
    "\"feature-extraction\": will return a FeatureExtractionPipeline.\n",
    "\"fill-mask\": will return a FillMaskPipeline:.\n",
    "\"image-classification\": will return a ImageClassificationPipeline.\n",
    "\"image-feature-extraction\": will return an ImageFeatureExtractionPipeline.\n",
    "\"image-segmentation\": will return a ImageSegmentationPipeline.\n",
    "\"image-to-image\": will return a ImageToImagePipeline.\n",
    "\"image-to-text\": will return a ImageToTextPipeline.\n",
    "\"mask-generation\": will return a MaskGenerationPipeline.\n",
    "\"object-detection\": will return a ObjectDetectionPipeline.\n",
    "\"question-answering\": will return a QuestionAnsweringPipeline.\n",
    "\"summarization\": will return a SummarizationPipeline.\n",
    "\"table-question-answering\": will return a TableQuestionAnsweringPipeline.\n",
    "\"text2text-generation\": will return a Text2TextGenerationPipeline.\n",
    "\"text-classification\" (alias \"sentiment-analysis\" available): will return a TextClassificationPipeline.\n",
    "\"text-generation\": will return a TextGenerationPipeline:.\n",
    "\"text-to-audio\" (alias \"text-to-speech\" available): will return a TextToAudioPipeline:.\n",
    "\"token-classification\" (alias \"ner\" available): will return a TokenClassificationPipeline.\n",
    "\"translation\": will return a TranslationPipeline.\n",
    "\"translation_xx_to_yy\": will return a TranslationPipeline.\n",
    "\"video-classification\": will return a VideoClassificationPipeline.\n",
    "\"visual-question-answering\": will return a VisualQuestionAnsweringPipeline.\n",
    "\"zero-shot-classification\": will return a ZeroShotClassificationPipeline.\n",
    "\"zero-shot-image-classification\": will return a ZeroShotImageClassificationPipeline.\n",
    "\"zero-shot-audio-classification\": will return a ZeroShotAudioClassificationPipeline.\n",
    "\"zero-shot-object-detection\": will return a ZeroShotObjectDetectionPipeline.\n",
    "```\n",
    "\n",
    "Let's start with ```text-classification``` tasks, which classify texts into a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb7691e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T15:47:01.319179Z",
     "start_time": "2024-09-08T15:46:35.493988Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318,
     "referenced_widgets": [
      "bb613ca475a04b6291ae555452ff32ae",
      "1f9467c8d3954898821783e01817a4d8",
      "51df8604620e4b0cbdd26073cef93e0a",
      "4cb75848f0c54ddeb2044cea6324b0b5",
      "8aa6e2f2a1b3491395bcc082607b0161",
      "9904c377b31f46988e998277d9e9a599",
      "29d18ac807854409ade63ae8bfaf3949",
      "51332c992c684534824ce2fdcc00a2c0",
      "c15e4d8258554edd8ba71bf66216e429",
      "a3ebfcf862414729bfd36a1cd2107798",
      "817809c0513e4ba891bf0ef06545c476",
      "df75dd1d690040398627192a410ed365",
      "3acf9ac64f8a4164bd6c8e2d8531992f",
      "cb0b7f3b3eb849759984f1ecdd7f8150",
      "f5339a7792d34d98bd79039b4fddc528",
      "3a1f57a752a241eba0f3eea13f1e0807",
      "c6497eb167d84ddc92a621022fbdba6b",
      "fee6752f4b9949f8bc7f88a755879c4b",
      "4dd38be5ab6040cdb2649f61f4610caa",
      "69feafa237de41efb0f1f6df1fa03644",
      "a2c50406950942f2a360145e1f8a1122",
      "dedbd73e87ee405da25a06848ad74be1",
      "856bd9b5bd2d48f0aa6db411eede6083",
      "52ac2333926743928c02d07225bb4cb0",
      "c67d335d413c4c73a77410d12041097e",
      "30bcbdfb83fb4103a8b891d448be4f45",
      "1d700c99ed3449d78b46c106c760cc41",
      "6a3a58181d7a4c70b776d32bb186431c",
      "5ff90183b7194101859bc69abaab5b3e",
      "1c1b95ee79ca40818edb8ff408969579",
      "014d2422730c4dc5ad1a9a3e9c860ff2",
      "cc18334aef434e77864ce6ff2be24f54",
      "474dc8864ce7441a9cb6baca6673d5b3",
      "8aa7c4d94a5946fc97d0fa035a2b8cef",
      "2cc7e30e4f2e431da61b8f00348056ce",
      "076958b62879480ba9ffd6af8f8df252",
      "78abf25f412a4bd1be24b688231a9a03",
      "1f835a06c38e4c3d8f9fbeeb81f3a5ef",
      "763e69488558483bb6e1dda8341a5dd8",
      "01b2dede303043ef9a467678a7aba5c1",
      "b35448dc110f4803b70c752aa3ffc75d",
      "82f121cea3a545b08d2719dbac50c8e6",
      "d6721980953e4a6698f185aa8d610104",
      "edda3719b30346e9b7f256ba83a242fc"
     ]
    },
    "id": "0cb7691e",
    "outputId": "e43f752f-bd4e-4d74-8edd-f8380cb191a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb613ca475a04b6291ae555452ff32ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df75dd1d690040398627192a410ed365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856bd9b5bd2d48f0aa6db411eede6083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa7c4d94a5946fc97d0fa035a2b8cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('text-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c7384",
   "metadata": {
    "id": "e07c7384"
   },
   "source": [
    "by default, hugging face will use the most-downloaded model from the hub for the given task. In this case, the default model is `distilbert/distilbert-base-uncased-finetuned-sst-2-english`, you can find more about this model on the following [link](distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
    "\n",
    "Nevertheless, we can use any model we want for text-classification. Search in the [hub](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending) for any model you want, I chose the `michellejieli/emotion_text_classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08173fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T15:50:27.699702Z",
     "start_time": "2024-09-08T15:50:27.695313Z"
    },
    "id": "d08173fb"
   },
   "outputs": [],
   "source": [
    "HUB_MODEL_NAME = 'michellejieli/emotion_text_classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccddd1ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T15:50:58.688723Z",
     "start_time": "2024-09-08T15:50:29.102428Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "4d16049306944b869e0f7e1200abdac7",
      "c5131a86bcbd4093bd93006ddd76d4d4",
      "3a2ecec64e2c4691861dd27edeeeb8a9",
      "28346f85e1e145a39888e78ed4dfd52a",
      "e7b1de4305fb45238160a0ee954fc97e",
      "06a170a9cc26416a9cbead37ffe822f8",
      "5c29dccd7c164b169f1cbbafe4d77161",
      "bfd1da0583394f63ab97bec6dd2f3777",
      "88827bf234574c1db3fb6a4597748db0",
      "8f572adfe99e43978b79628ea3c33a08",
      "4332cbf7240b449d9f375366046baeb7",
      "0850b8107de341bd99cdb7aa1691e4e8",
      "c7e7019b646940fbb6a873ea598b31ef",
      "7c48b258981345ea914e4f94bd00a76c",
      "0c39611ba74245ccb3a1c0909ab1a99c",
      "f8150489a2b342ceb18f8b3f16e947f3",
      "6af077ef03c043a79c685dcb58ccadc6",
      "99dd08bd3a8841998cd784ff2e77e2e2",
      "b8a145f2e6b140fdb9cdf3ded724e24f",
      "ffe3d5846ac940528a82e598c1753ee1",
      "a730037fb2d7474a8e67b679dc6860cf",
      "21626a94f9bb4e609e48fc52cb3f0476",
      "e5519ee39dfc4176925cccc0a3d5e7b6",
      "a0092ce306804caf8bf3caa4babbd39f",
      "dfffbd19b5e64a92843341e6f86e5930",
      "b042330b018342c1b7dd462a7ca124d1",
      "0525d3d23eb64e29bf49e4908b17a4eb",
      "9cf4e7fff1434249909f4ebeddaaa26b",
      "e136b4789162494293517e001aa889df",
      "03057d5257454692a9d4bc86ab1442ab",
      "368760e67f954e458c443c8dabaa8e9b",
      "1572fdc592d043108279c52c7fd18732",
      "4a0dcccf6bb24b1f9a6d4d922ac42e56",
      "5ce08eb2a4ba4dc3bb92a456402134cd",
      "8478f9d44d4f46fbb327b9a8662712e9",
      "f86c17399ccf424f892e7564fe18c75d",
      "65dcba929033402cb957f8700202503d",
      "dadee9a936454bbc8c8ab99fadc8953d",
      "82370a0454f045f4879378be0d0701fa",
      "02bc2f8c53e2435cb3c10f8472b406f0",
      "7d2c2f9cc5fa4788a2b62b02abedcc57",
      "926af36f9ff44ddeb3a06678b1f21282",
      "6f2cbf4e78e145469ce39674942e917a",
      "cb003fbe5fc04c73a62e2e75016765e6",
      "34ddee1458e4405e89ba74b51a195272",
      "4caf02f1f81b405196e9b62624c11f8b",
      "54ab28977e834fb09c4341c3b964aae7",
      "60e8b4c55dbc4580b836fd2dc79c9df9",
      "f410bd95b5454acaa728eaf035fe1216",
      "04448c6ed0154e8196518020572ba3d8",
      "7ec996c1134d4bac82d09dc3b8149ac0",
      "6f155406149547f4bef1484e343c3fd2",
      "0cb8c345f6ad4e75be18e3a31c133737",
      "4a4b1e77b6334f94a8954395c7506173",
      "f01a27dd333540beb9b68f3f435f5cb6",
      "26b013c2a4424ba8bd99ab4a672279aa",
      "120687fe5586456baaad278f4a8d787b",
      "0c97c2f0eb894f568d2a5a366036c2c9",
      "bd7912e5716d4a4496806739ebf0233b",
      "16cb0b8845cf4d9d8b231cb3eded4360",
      "b2d890194002449fa393c16ff38334b7",
      "cb18a3d8ae84421caf0a1ad7c388c919",
      "3c9fd86931024352a71a490af3b84435",
      "a7a8c3e223cf498884092ea787269655",
      "6997e7ec1b654a89be11eb519c4b7fa5",
      "466000bc59ee490f858c9f69edc71321",
      "f9487622c74147b1ad178877140d7535",
      "eab352c695ac4d2aba6cce4cf7358189",
      "9085cf25b53c4c009b8609e7a7981ead",
      "6ef6e08cbfb946cd9cf72d4a034cfb92",
      "dd0443f7d4c2437684b84631fdfb80a4",
      "3cb23fd5c4df4c9fada99bbdd8085196",
      "793b9c3ac241467680cb34947556a190",
      "ede7cd48b1e1449fbf173b8800d6963f",
      "f35f28368b96404ea6f3d5112c0a4330",
      "9439b1a188d441e5893c934888cdd30a",
      "e33c61d738f643d98c97743bad1daa20",
      "b08529fe937440298a533d52733acc6c",
      "e8e5dd4cbb8443aaa843fc8ba2bb5252",
      "375b95c991b4414b8a964837c4a1df61",
      "3b51714baee74f038960ee58f6890d8b",
      "a9f96b2694bd4df0a35822aa8a24061d",
      "23b1275ce9e74e9fbe9826d6f687941e",
      "e68b1ffaac58464daff47a0548de5811",
      "3fe295fda31f4380b5bcf2042527fe31",
      "1aec2b04cfda4dacbe6fd9b166740291",
      "c2b6c298d37848a2abcf16bd0ab03dc9",
      "29e81e1641a64ffda6246503afcb2c80"
     ]
    },
    "id": "ccddd1ad",
    "outputId": "c92197a7-2397-4a4d-8028-d90473c4f7d8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d16049306944b869e0f7e1200abdac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0850b8107de341bd99cdb7aa1691e4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5519ee39dfc4176925cccc0a3d5e7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/413 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce08eb2a4ba4dc3bb92a456402134cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ddee1458e4405e89ba74b51a195272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b013c2a4424ba8bd99ab4a672279aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9487622c74147b1ad178877140d7535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08529fe937440298a533d52733acc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('text-classification', model=HUB_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1133cb8",
   "metadata": {
    "id": "d1133cb8"
   },
   "source": [
    "let's try it out!. Since we are dealing with text-classification, we just have to call the ```classifier``` with the text we want to classifity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bea980",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T15:51:59.221191Z",
     "start_time": "2024-09-08T15:51:59.182116Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "d5bea980",
    "outputId": "37dca410-5a5a-4fe7-aa0d-4f0c5a449db2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'joy', 'score': 0.9909875988960266}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('I love huggingface, it is amazing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101202cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T15:52:32.786809Z",
     "start_time": "2024-09-08T15:52:32.713965Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "101202cc",
    "outputId": "2039cc64-3006-4620-bc00-8f9c49903b82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'sadness', 'score': 0.47510215640068054}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('huggingface is great, but I do not understand how to use it :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a6316",
   "metadata": {
    "id": "838a6316"
   },
   "source": [
    "as we can see, our classifier returns a structured output with the associed label and score of the input text. Later, we will use this predictions to evaluate the performance of our model, but for now, let's keep learning how to do inference!\n",
    "\n",
    "\n",
    "We can classify multiples sentences in a single call as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9816f242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:36:37.380614Z",
     "start_time": "2024-09-08T16:36:37.320125Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9816f242",
    "outputId": "10da0d2c-94ae-41c5-c992-f93a73dbb0fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'joy', 'score': 0.9909875988960266},\n",
       " {'label': 'sadness', 'score': 0.47510215640068054}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\n",
    "    'I love huggingface, it is amazing!',\n",
    "    'huggingface is great, but I do not understand how to use it :('\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a25dc",
   "metadata": {
    "id": "ae2a25dc"
   },
   "source": [
    "###  Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8615a7",
   "metadata": {
    "id": "1d8615a7"
   },
   "source": [
    "By default, this sentences are processed sequentially, which can be extremly slowly for longer sequences of texts.\n",
    "to enable batch processing, we specify the ```batch_size``` parameter in the initialization of the pipeline.\n",
    "\n",
    "Nevertheless, batching is not necessarily faster, and can actually be quite slower in some cases. You can read more about it in [here](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "463dddad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:38:00.648847Z",
     "start_time": "2024-09-08T16:38:00.034842Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "463dddad",
    "outputId": "8f2f4119-8ffd-4142-82e9-eef4ec4e34f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('text-classification', model=HUB_MODEL_NAME, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db2e4a87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:38:04.606414Z",
     "start_time": "2024-09-08T16:38:04.545798Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db2e4a87",
    "outputId": "8210f046-8e9a-4fb4-baa5-e265082f0184"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'joy', 'score': 0.9909875988960266},\n",
       " {'label': 'sadness', 'score': 0.4751024842262268}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\n",
    "    'I love huggingface, it is amazing!',\n",
    "    'huggingface is great, but I do not understand how to use it :('\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e34f6",
   "metadata": {
    "id": "8e2e34f6"
   },
   "source": [
    "###  Device\n",
    "Pipelines work both for CPU or GPU. the `device` allow us to specify in which device to run the model on. This work regarless of whether you are using Pytorch or Tensorflow.\n",
    "\n",
    "you can set device to `device=n` to specific device, for example:\n",
    "- `device=0` uses the first GPU\n",
    "- `device=1` uses the second GPU\n",
    "- `device=cpu` uses the CPU.\n",
    "\n",
    "Alternatively, you can set `device_map=\"auto\"` to automatically loads and stores model weights across devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b200868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:45:00.987342Z",
     "start_time": "2024-09-08T16:45:00.341503Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4b200868",
    "outputId": "644b98bf-bd56-4ee3-ad5d-a286fc2f7887"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('text-classification', model=HUB_MODEL_NAME, batch_size=2, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9538f3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:45:05.504509Z",
     "start_time": "2024-09-08T16:45:05.498194Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9538f3e",
    "outputId": "51fffb28-0753-4894-d323-52300f3c62a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d16bde7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:45:11.573243Z",
     "start_time": "2024-09-08T16:45:11.520984Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d16bde7c",
    "outputId": "94ad9bf3-402c-4839-fc91-38a02ba83491"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'joy', 'score': 0.9909875988960266},\n",
       " {'label': 'sadness', 'score': 0.4751022160053253}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\n",
    "    'I love huggingface, it is amazing!',\n",
    "    'huggingface is great, but I do not understand how to use it :('\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f418c8f2",
   "metadata": {
    "id": "f418c8f2"
   },
   "source": [
    "### Pipelines on a dataset\n",
    "Pipelines can also be executed on large datasets. A dataset can be any iterable object in python, but it is optimal to use a generator, to avoid allocating memory for the whole dataset and you can feed the GPU/CPU as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be4fdf76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:51:59.765053Z",
     "start_time": "2024-09-08T16:51:59.758564Z"
    },
    "id": "be4fdf76"
   },
   "outputs": [],
   "source": [
    "def dataset_generator():\n",
    "    for i in range(25):\n",
    "        yield f'This is the {i}th time I told you to use generators!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35e8014c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:52:00.172796Z",
     "start_time": "2024-09-08T16:52:00.166533Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35e8014c",
    "outputId": "15362f76-992e-4e5a-92d6-69e90d92bf87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the 0th time I told you to use generators!',\n",
       " 'This is the 1th time I told you to use generators!',\n",
       " 'This is the 2th time I told you to use generators!',\n",
       " 'This is the 3th time I told you to use generators!',\n",
       " 'This is the 4th time I told you to use generators!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_iter = dataset_generator()\n",
    "[next(dataset_iter) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c23eb66b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:52:00.362820Z",
     "start_time": "2024-09-08T16:52:00.358757Z"
    },
    "id": "c23eb66b"
   },
   "outputs": [],
   "source": [
    "# this returns a generator!\n",
    "predictions_generator = classifier(dataset_generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4abc7b81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:52:00.565414Z",
     "start_time": "2024-09-08T16:52:00.560265Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4abc7b81",
    "outputId": "82f9ab43-0a31-444a-a92a-cb0ca5232c9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.pt_utils.PipelineIterator at 0x7e0565181090>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea222c87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:52:01.165888Z",
     "start_time": "2024-09-08T16:52:00.757660Z"
    },
    "id": "ea222c87"
   },
   "outputs": [],
   "source": [
    "# iterate over the dataset. This is where the inference occurs\n",
    "predictions = list(predictions_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d2d531c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:52:01.173551Z",
     "start_time": "2024-09-08T16:52:01.168586Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "0d2d531c",
    "outputId": "8e9ea7d3-898e-436c-feca-6f577d304882"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'anger', 'score': 0.5030127763748169},\n",
       " {'label': 'anger', 'score': 0.39345771074295044},\n",
       " {'label': 'anger', 'score': 0.3661690056324005}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e71345c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T16:52:01.245795Z",
     "start_time": "2024-09-08T16:52:01.239818Z"
    },
    "id": "2e71345c"
   },
   "outputs": [],
   "source": [
    "assert len(predictions) == 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ff7e5a8",
   "metadata": {
    "id": "7ff7e5a8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
